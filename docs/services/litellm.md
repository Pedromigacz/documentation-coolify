---
title: "Litellm"
description: "Deploy LiteLLM proxy on Coolify for unified LLM API with 100+ models, load balancing, fallbacks, and standardized OpenAI-compatible interface."
---

# What is Litellm?

<ZoomableImage src="/docs/images/services/litellm1.avif" alt="Litellm dashboard" />

LiteLLM is an open-source LLM Gateway to manage authentication, loadbalancing, and spend tracking across 100+ LLMs. All in the OpenAI format.

## Screenshots

<br />
<ZoomableImage src="/docs/images/services/litellm2.avif" alt="Litellm dashboard" />
<br />
<ZoomableImage src="/docs/images/services/litellm3.avif" alt="Litellm dashboard" />
<br />
<ZoomableImage src="/docs/images/services/litellm4.avif" alt="Litellm dashboard" />

## Links

- [The official website](https://docs.litellm.ai?utm_source=coolify.io)
