---
title: "Ollama"
description: "Run Ollama on Coolify for local LLM hosting supporting Llama, Mistral, Gemma, and custom models with REST API for AI applications."
---

<ZoomableImage src="/docs/images/services/ollama-logo.webp" alt="Ollama dashboard" />

## What is Ollama?

Ollama is a lightweight and efficient server for running large language models (LLMs) on your local machine or in the cloud.

It includes OpenWebUI, a web-based interface for interacting with the models.

## Screenshots

<ZoomableImage src="/docs/images/services/ollama.gif" alt="Ollama dashboard" />

## Links

- [The official website](https://ollama.com/?utm_source=coolify.io)
- [GitHub](https://github.com/ollama/ollama?utm_source=coolify.io)
